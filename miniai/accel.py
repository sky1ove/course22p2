# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/17_DDPM_v2.ipynb.

# %% auto 0
__all__ = ['MixedPrecision', 'AccelerateCB', 'FabricCB', 'MultDL']

# %% ../nbs/17_DDPM_v2.ipynb 3
import pickle,gzip,math,os,time,shutil,torch,random,logging
import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from collections.abc import Mapping
from pathlib import Path
from functools import partial

from fastcore.foundation import L
import torchvision.transforms.functional as TF,torch.nn.functional as F
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torch.optim import lr_scheduler

from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .init import *
from .sgd import *
from .resnet import *
from .augment import *

# %% ../nbs/17_DDPM_v2.ipynb 41
class MixedPrecision(TrainCB):
    order = DeviceCB.order+10
    
    def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler()

    def before_batch(self, learn):
        self.autocast = torch.autocast("cuda", dtype=torch.float16)
        self.autocast.__enter__()

    def after_loss(self, learn): self.autocast.__exit__(None, None, None)
        
    def backward(self, learn): self.scaler.scale(learn.loss).backward()

    def step(self, learn):
        self.scaler.step(learn.opt)
        self.scaler.update()

# %% ../nbs/17_DDPM_v2.ipynb 49
from accelerate import Accelerator

# %% ../nbs/17_DDPM_v2.ipynb 50
class AccelerateCB(TrainCB):
    order = DeviceCB.order+10
    def __init__(self, n_inp=1, mixed_precision="fp16",fname='model'):
        super().__init__(n_inp=n_inp)
        self.acc = Accelerator(mixed_precision=mixed_precision)
        self.fname=fname
        
    def before_fit(self, learn):
        learn.model,learn.opt,learn.dls.train,learn.dls.valid = self.acc.prepare(
            learn.model, learn.opt, learn.dls.train, learn.dls.valid)

    def backward(self, learn): self.acc.backward(learn.loss)
    def after_fit(self,learn):
        self.acc.wait_for_everyone()
        if self.acc.is_main_process:
            unwrapped_model = self.acc.unwrap_model(learn.model)
            self.acc.save_model(unwrapped_model, self.fname)

# %% ../nbs/17_DDPM_v2.ipynb 58
from lightning.fabric import Fabric

# %% ../nbs/17_DDPM_v2.ipynb 59
class FabricCB(TrainCB):
    order = DeviceCB.order+10
    """
    1. When using FabricCB for distributed training, remove DeviceCB()
    2. Work in python script; use python XX.py to run
    Notes: it is not compatible with MultiDL trick; remember to not to include it
    
    """
    
    def __init__(self,n_inp=1, **kwargs):
        super().__init__(n_inp = n_inp)
        self.fabric=Fabric(**kwargs)
        self.fabric.launch()
        
    def before_fit(self, learn):
        learn.dls.train=self.fabric.setup_dataloaders(learn.dls.train)

        # in case sometimes valid is empty
        try: learn.dls.valid=self.fabric.setup_dataloaders(learn.dls.valid)
        except: pass
        learn.model,learn.opt = self.fabric.setup(learn.model,learn.opt)
        

    def backward(self, learn): self.fabric.backward(learn.loss)

# %% ../nbs/17_DDPM_v2.ipynb 61
class MultDL:
    def __init__(self, dl, mult=2): self.dl,self.mult = dl,mult
    def __len__(self): return len(self.dl)*self.mult
    def __iter__(self):
        for o in self.dl:
            for i in range(self.mult): yield o
